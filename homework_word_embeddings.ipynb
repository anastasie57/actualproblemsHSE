{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a98604cb",
      "metadata": {
        "id": "a98604cb"
      },
      "source": [
        "Домашку будет легче делать в колабе (убедитесь, что у вас runtype с gpu)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "JRqI_0rPKH6T",
      "metadata": {
        "id": "JRqI_0rPKH6T"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from string import punctuation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "import re\n",
        "\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c422aa0",
      "metadata": {
        "id": "9c422aa0"
      },
      "source": [
        "# Задание 1 (3 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4a72790",
      "metadata": {
        "id": "e4a72790"
      },
      "source": [
        "Обучите word2vec модели с негативным семплированием (cbow и skip-gram) с помощью tensorflow аналогично тому, как это было сделано в семинаре. Вам нужно изменить следующие пункты:\n",
        "1) добавьте лемматизацию в предобработку (любым способом)  \n",
        "2) измените размер окна в большую или меньшую сторону\n",
        "3) измените размерность итоговых векторов\n",
        "\n",
        "Выберете несколько не похожих по смыслу слов (не таких как в семинаре), и протестируйте полученные эмбединги (найдите ближайшие слова и оцените качество, как в семинаре).\n",
        "Постарайтесь обучать модели как можно дольше и на как можно большем количестве данных. (Но если у вас мало времени или ресурсов, то допустимо взять поменьше данных и поставить меньше эпох)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "y3eB7v1oMk9m",
      "metadata": {
        "id": "y3eB7v1oMk9m"
      },
      "outputs": [],
      "source": [
        "from pymystem3 import Mystem\n",
        "import os, json\n",
        "mystem = Mystem()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cde5fd96",
      "metadata": {
        "id": "cde5fd96"
      },
      "outputs": [],
      "source": [
        "def preprocess(text):\n",
        "    tokens = re.sub('#+', ' ', text.lower()).split()\n",
        "    tokens = [token.strip(punctuation) for token in tokens]\n",
        "    tokens = [token for token in tokens if token]\n",
        "    tokens = [mystem.lemmatize(token)[0] for token in tokens if token and mystem.lemmatize(token)[0] != '—\\n']\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f71d7cb",
      "metadata": {
        "id": "4f71d7cb"
      },
      "outputs": [],
      "source": [
        "wiki = open('wiki_data.txt').read().split('\\n')\n",
        "vocab = Counter()\n",
        "\n",
        "for text in wiki:\n",
        "  vocab.update(preprocess(text))\n",
        "\n",
        "filtered_vocab = set()\n",
        "\n",
        "for word in vocab:\n",
        "    if vocab[word] > 30:\n",
        "        filtered_vocab.add(word)\n",
        "\n",
        "word2id = {'PAD':0}\n",
        "\n",
        "for word in filtered_vocab:\n",
        "    word2id[word] = len(word2id)\n",
        "\n",
        "id2word = {i:word for word, i in word2id.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4eUrpFDDO-Vg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "4eUrpFDDO-Vg",
        "outputId": "24ce2942-e8e2-49e0-e656-23067d124766"
      },
      "outputs": [
        {
          "ename": "BrokenPipeError",
          "evalue": "[Errno 32] Broken pipe",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b5e023c37c95>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwiki\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-fbf090c0622b>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'—\\n'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-fbf090c0622b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'—\\n'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pymystem3/mystem.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mneed_encode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lemma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pymystem3/mystem.py\u001b[0m in \u001b[0;36manalyze\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_analyze_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pymystem3/mystem.py\u001b[0m in \u001b[0;36m_analyze_impl\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_mystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_procin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_procin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_NL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_procin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
          ]
        }
      ],
      "source": [
        "sentences = []\n",
        "\n",
        "for text in wiki:\n",
        "    tokens = preprocess(text)\n",
        "    if not tokens:\n",
        "        continue\n",
        "    ids = [word2id[token] for token in tokens if token in word2id]\n",
        "    sentences.append(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Eqz_VBHpS1Id",
      "metadata": {
        "id": "Eqz_VBHpS1Id"
      },
      "source": [
        "Skip Gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zNmOmE9iPpfW",
      "metadata": {
        "id": "zNmOmE9iPpfW"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "window = 10\n",
        "for sent in sentences[:1000]:\n",
        "    for i in range(len(sent)-1):\n",
        "        word = sent[i] # target\n",
        "        context = sent[max(0, i-window):i] + sent[i+1:i+window]  # context (слова до и после целевого)\n",
        "\n",
        "        for context_word in context:\n",
        "            X.append(word)\n",
        "            y.append(context_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BDAvP6--Pt2t",
      "metadata": {
        "id": "BDAvP6--Pt2t"
      },
      "outputs": [],
      "source": [
        "X[1], y[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WEbfNlz_PwYB",
      "metadata": {
        "id": "WEbfNlz_PwYB"
      },
      "outputs": [],
      "source": [
        "len(X), len(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g3H1xUeRPzE4",
      "metadata": {
        "id": "g3H1xUeRPzE4"
      },
      "outputs": [],
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z-VfgsPnP9q0",
      "metadata": {
        "id": "z-VfgsPnP9q0"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Embedding(input_dim=len(word2id),\n",
        "                                    input_length=1,\n",
        "                                    output_dim=150))\n",
        "\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "model.add(tf.keras.layers.Dense(len(word2id),\n",
        "                                activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss='sparse_categorical_crossentropy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oRi-s4sYQghn",
      "metadata": {
        "id": "oRi-s4sYQghn"
      },
      "outputs": [],
      "source": [
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3_0smG4WSGPt",
      "metadata": {
        "id": "3_0smG4WSGPt"
      },
      "source": [
        "Обучаем модель."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4jOslpKPR8dZ",
      "metadata": {
        "id": "4jOslpKPR8dZ"
      },
      "outputs": [],
      "source": [
        "model.fit(X_train, y_train,\n",
        "          validation_data=(X_valid, y_valid),\n",
        "          batch_size=1000,\n",
        "          epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZiZRNDi6SE7r",
      "metadata": {
        "id": "ZiZRNDi6SE7r"
      },
      "outputs": [],
      "source": [
        "print(model.history.history.keys())\n",
        "plt.plot(model.history.history['loss'])\n",
        "plt.plot(model.history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AxVQmie3S81S",
      "metadata": {
        "id": "AxVQmie3S81S"
      },
      "outputs": [],
      "source": [
        "model.layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6LVAALWTCmz",
      "metadata": {
        "id": "b6LVAALWTCmz"
      },
      "outputs": [],
      "source": [
        "model.layers[0].get_weights()[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RSnh27tgTFv1",
      "metadata": {
        "id": "RSnh27tgTFv1"
      },
      "outputs": [],
      "source": [
        "model.layers[2].get_weights()[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yVgWxjZuTIZC",
      "metadata": {
        "id": "yVgWxjZuTIZC"
      },
      "outputs": [],
      "source": [
        "embeddings_1 = model.layers[0].get_weights()[0]\n",
        "embeddings_2 = model.layers[2].get_weights()[0].T\n",
        "\n",
        "embeddings = np.mean([embeddings_1, embeddings_2], axis=0)\n",
        "\n",
        "embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YLv2GgY1TSpm",
      "metadata": {
        "id": "YLv2GgY1TSpm"
      },
      "outputs": [],
      "source": [
        "def most_similar(word, embeddings):\n",
        "    similar = [id2word[i] for i in\n",
        "               cosine_distances(embeddings[word2id[word]].reshape(1, -1), embeddings).argsort()[0][:10]]\n",
        "    return similar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KAqwrpHRTMZ8",
      "metadata": {
        "id": "KAqwrpHRTMZ8"
      },
      "outputs": [],
      "source": [
        "most_similar('машина', embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "O1M7e3aqTNgY",
      "metadata": {
        "id": "O1M7e3aqTNgY"
      },
      "outputs": [],
      "source": [
        "most_similar('цветок', embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wPyVn6VsTf3j",
      "metadata": {
        "id": "wPyVn6VsTf3j"
      },
      "source": [
        "CBOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zIkAh4-STjAv",
      "metadata": {
        "id": "zIkAh4-STjAv"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "window = 10\n",
        "for sent in sentences[:10000]:\n",
        "    for i in range(len(sent)-1):\n",
        "        word = sent[i]\n",
        "        context = sent[max(0, i-window):i] + sent[i+1:i+window]\n",
        "\n",
        "        X.append(context)\n",
        "        y.append(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7qcbeiXOTrcj",
      "metadata": {
        "id": "7qcbeiXOTrcj"
      },
      "outputs": [],
      "source": [
        "X[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HAHl1UowTwLc",
      "metadata": {
        "id": "HAHl1UowTwLc"
      },
      "outputs": [],
      "source": [
        "X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=10, padding='post')\n",
        "y = np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ropTCJkGTw8k",
      "metadata": {
        "id": "ropTCJkGTw8k"
      },
      "outputs": [],
      "source": [
        "# контексты после паддинга\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ADOR_Af5T7L-",
      "metadata": {
        "id": "ADOR_Af5T7L-"
      },
      "outputs": [],
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.05)\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Embedding(input_dim=len(word2id),\n",
        "                                    input_length=10,\n",
        "                                    output_dim=150))\n",
        "\n",
        "model.add(tf.keras.layers.Lambda(lambda x: tf.keras.backend.sum(x, axis=1)))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(len(word2id), activation='softmax'))\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001), loss='sparse_categorical_crossentropy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UrSAF-bNUPzL",
      "metadata": {
        "id": "UrSAF-bNUPzL"
      },
      "outputs": [],
      "source": [
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "X7uBMf7_UfvE",
      "metadata": {
        "id": "X7uBMf7_UfvE"
      },
      "source": [
        "Обучаем модель."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wibrdy1fUWPH",
      "metadata": {
        "id": "wibrdy1fUWPH"
      },
      "outputs": [],
      "source": [
        "model.fit(X_train, y_train,\n",
        "          validation_data=(X_valid, y_valid),\n",
        "          batch_size=1000,\n",
        "          epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Rk5eslnPUeac",
      "metadata": {
        "id": "Rk5eslnPUeac"
      },
      "outputs": [],
      "source": [
        "print(model.history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(model.history.history['loss'])\n",
        "plt.plot(model.history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i2uHRIIpUpVd",
      "metadata": {
        "id": "i2uHRIIpUpVd"
      },
      "outputs": [],
      "source": [
        "embeddings_1 = model.layers[0].get_weights()[0]\n",
        "embeddings_2 = model.layers[2].get_weights()[0].T\n",
        "\n",
        "embeddings = np.mean([embeddings_1, embeddings_2], axis=0)\n",
        "\n",
        "embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZpLtFt1jU1R_",
      "metadata": {
        "id": "ZpLtFt1jU1R_"
      },
      "outputs": [],
      "source": [
        "most_similar('машина', embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Wn_I808QU2Bo",
      "metadata": {
        "id": "Wn_I808QU2Bo"
      },
      "outputs": [],
      "source": [
        "most_similar('цветок', embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-nY4S8C1VQLD",
      "metadata": {
        "id": "-nY4S8C1VQLD"
      },
      "source": [
        "Добавляем негативное семплирование"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adH2y-xtVPnR",
      "metadata": {
        "id": "adH2y-xtVPnR"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(id2word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pQ4ozRtLVmIK",
      "metadata": {
        "id": "pQ4ozRtLVmIK"
      },
      "outputs": [],
      "source": [
        "def gen_batches_sg(sentences, window = 10, batch_size=1000):\n",
        "\n",
        "    left_context_length = (window/2).__ceil__()\n",
        "    right_context_length = window // 2\n",
        "\n",
        "    while True:\n",
        "        X_target = []\n",
        "        X_context = []\n",
        "        y = []\n",
        "\n",
        "        for sent in sentences:\n",
        "            for i in range(len(sent)-1):\n",
        "                word = sent[i]\n",
        "                context = sent[max(0, i-left_context_length):i] + sent[i+1:i+right_context_length]\n",
        "                for context_word in context:\n",
        "                    X_target.append(word)\n",
        "                    X_context.append(context_word)\n",
        "                    y.append(1)\n",
        "\n",
        "                    X_target.append(word)\n",
        "                    X_context.append(np.random.randint(vocab_size))\n",
        "                    y.append(0)\n",
        "\n",
        "                    if len(X_target) >= batch_size:\n",
        "                        X_target = np.array(X_target)\n",
        "                        X_context = np.array(X_context)\n",
        "                        y = np.array(y)\n",
        "                        yield ((X_target, X_context), y)\n",
        "                        X_target = []\n",
        "                        X_context = []\n",
        "                        y = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vfLYrFKoVu80",
      "metadata": {
        "id": "vfLYrFKoVu80"
      },
      "outputs": [],
      "source": [
        "def gen_batches_cbow(sentences, window = 5, batch_size=1000):\n",
        "\n",
        "    left_context_length = (window/2).__ceil__()\n",
        "    right_context_length = window // 2\n",
        "\n",
        "    while True:\n",
        "        X_target = []\n",
        "        X_context = []\n",
        "        y = []\n",
        "\n",
        "        for sent in sentences:\n",
        "            for i in range(len(sent)-1):\n",
        "                word = sent[i]\n",
        "                context = sent[max(0, i-left_context_length):i] + sent[i+1:i+right_context_length]\n",
        "\n",
        "                X_target.append(word)\n",
        "                X_context.append(context)\n",
        "                y.append(1)\n",
        "\n",
        "                X_target.append(np.random.randint(vocab_size))\n",
        "                X_context.append(context)\n",
        "                y.append(0)\n",
        "\n",
        "                if len(X_target) == batch_size:\n",
        "                    X_target = np.array(X_target)\n",
        "                    X_context = tf.keras.preprocessing.sequence.pad_sequences(X_context, maxlen=window)\n",
        "                    y = np.array(y)\n",
        "                    yield ((X_target, X_context), y)\n",
        "                    X_target = []\n",
        "                    X_context = []\n",
        "                    y = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vI6oz0n1WOfK",
      "metadata": {
        "id": "vI6oz0n1WOfK"
      },
      "outputs": [],
      "source": [
        "inputs_target = tf.keras.layers.Input(shape=(1,))\n",
        "inputs_context = tf.keras.layers.Input(shape=(1,))\n",
        "\n",
        "\n",
        "embeddings_target = tf.keras.layers.Embedding(input_dim=len(word2id), output_dim=300)(inputs_target, )\n",
        "embeddings_context = tf.keras.layers.Embedding(input_dim=len(word2id), output_dim=300)(inputs_context, )\n",
        "\n",
        "target = tf.keras.layers.Flatten()(embeddings_target)\n",
        "context = tf.keras.layers.Flatten()(embeddings_context)\n",
        "\n",
        "dot = tf.keras.layers.Dot(1)([target, context])\n",
        "outputs = tf.keras.layers.Activation(activation='sigmoid')(dot)\n",
        "\n",
        "model = tf.keras.Model(inputs=[inputs_target, inputs_context],\n",
        "                       outputs=outputs)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "II2_nid-Wx7i",
      "metadata": {
        "id": "II2_nid-Wx7i"
      },
      "source": [
        "Обучаем модель."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BtG8MzsdWmR-",
      "metadata": {
        "id": "BtG8MzsdWmR-"
      },
      "outputs": [],
      "source": [
        "model.fit(gen_batches_sg(sentences[:19000], window=5),\n",
        "          validation_data=gen_batches_sg(sentences[19000:],  window=10),\n",
        "          batch_size=1000,\n",
        "          steps_per_epoch=10000,\n",
        "          validation_steps=30,\n",
        "          epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbesjKkcWw2v",
      "metadata": {
        "id": "dbesjKkcWw2v"
      },
      "outputs": [],
      "source": [
        "embeddings = model.layers[2].get_weights()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j6havDQFW7TE",
      "metadata": {
        "id": "j6havDQFW7TE"
      },
      "outputs": [],
      "source": [
        "most_similar('машина', embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2SjafeBWW8KE",
      "metadata": {
        "id": "2SjafeBWW8KE"
      },
      "outputs": [],
      "source": [
        "most_similar('цветок', embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-lOmgQbeXEWd",
      "metadata": {
        "id": "-lOmgQbeXEWd"
      },
      "outputs": [],
      "source": [
        "#cbow negative sampling\n",
        "inputs_target = tf.keras.layers.Input(shape=(1,))\n",
        "inputs_context = tf.keras.layers.Input(shape=(10,))\n",
        "\n",
        "\n",
        "embeddings_target = tf.keras.layers.Embedding(input_dim=len(word2id), output_dim=300)(inputs_target, )\n",
        "embeddings_context = tf.keras.layers.Embedding(input_dim=len(word2id), output_dim=300)(inputs_context, )\n",
        "\n",
        "target = tf.keras.layers.Flatten()(embeddings_target)\n",
        "context = tf.keras.layers.Lambda(lambda x: tf.keras.backend.sum(x, axis=1))(embeddings_context)\n",
        "dot = tf.keras.layers.Dot(1)([target, context])\n",
        "\n",
        "outputs = tf.keras.layers.Activation(activation='sigmoid')(dot)\n",
        "\n",
        "model = tf.keras.Model(inputs=[inputs_target, inputs_context],\n",
        "                       outputs=outputs)\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yyNvq7XVXLwh",
      "metadata": {
        "id": "yyNvq7XVXLwh"
      },
      "source": [
        "Обучаем модель."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "349qpfeHXIZz",
      "metadata": {
        "id": "349qpfeHXIZz"
      },
      "outputs": [],
      "source": [
        "model.fit(gen_batches_cbow(sentences[:19000], window=5),\n",
        "          validation_data=gen_batches_cbow(sentences[19000:],  window=10),\n",
        "          batch_size=1000,\n",
        "          steps_per_epoch=5000,\n",
        "          validation_steps=30,\n",
        "          epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zOEwoaOeXLhi",
      "metadata": {
        "id": "zOEwoaOeXLhi"
      },
      "outputs": [],
      "source": [
        "embeddings = model.layers[2].get_weights()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oDs0AQcfXRT8",
      "metadata": {
        "id": "oDs0AQcfXRT8"
      },
      "outputs": [],
      "source": [
        "most_similar('машина', embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u0BvB656XTSR",
      "metadata": {
        "id": "u0BvB656XTSR"
      },
      "outputs": [],
      "source": [
        "most_similar('цветок', embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3b61b7c",
      "metadata": {
        "id": "c3b61b7c"
      },
      "source": [
        "# Задание 2 (3 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66eff080",
      "metadata": {
        "id": "66eff080"
      },
      "source": [
        "Обучите 1 word2vec и 1 fastext модель в gensim. В каждой из модели нужно задать все параметры, которые мы разбирали на семинаре. Заданные значения должны отличаться от дефолтных и от тех, что мы использовали на семинаре."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "986c2018",
      "metadata": {
        "id": "986c2018"
      },
      "outputs": [],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "F6Ar0JT9YH4k",
      "metadata": {
        "id": "F6Ar0JT9YH4k"
      },
      "outputs": [],
      "source": [
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5035bdc",
      "metadata": {
        "id": "e5035bdc"
      },
      "outputs": [],
      "source": [
        "texts = [preprocess(text) for text in wiki]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JQ9_NsysX9XN",
      "metadata": {
        "id": "JQ9_NsysX9XN"
      },
      "outputs": [],
      "source": [
        "w2v = gensim.models.Word2Vec(texts,\n",
        "                             cbow_mean=0,\n",
        "                             ns_exponent=0.74,\n",
        "                             sample=2e-5,\n",
        "                             vector_size=200,\n",
        "                             min_count=25,\n",
        "                             max_vocab_size=10000,\n",
        "                             window=10,\n",
        "                             epochs=5,\n",
        "                             sg=1,\n",
        "                             negative=13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TqNVMduaYPhg",
      "metadata": {
        "id": "TqNVMduaYPhg"
      },
      "outputs": [],
      "source": [
        "w2v.wv.most_similar('машина')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h_RKJJpLYHLg",
      "metadata": {
        "id": "h_RKJJpLYHLg"
      },
      "outputs": [],
      "source": [
        "w2v.wv.most_similar('цветок')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u-x0WDM3YSbD",
      "metadata": {
        "id": "u-x0WDM3YSbD"
      },
      "outputs": [],
      "source": [
        "ft = gensim.models.FastText(texts, min_n=2, max_n=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-tq2lgNeYuw-",
      "metadata": {
        "id": "-tq2lgNeYuw-"
      },
      "outputs": [],
      "source": [
        "ft.wv.most_similar('машина')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IcXaJKlqYwbN",
      "metadata": {
        "id": "IcXaJKlqYwbN"
      },
      "outputs": [],
      "source": [
        "ft.wv.most_similar('цветок')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4bb928c",
      "metadata": {
        "id": "e4bb928c"
      },
      "source": [
        "# Задание 3 (4 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3019b0d1",
      "metadata": {
        "id": "3019b0d1"
      },
      "source": [
        "Используя датасет для классификации (labeled.csv) и простую нейронную сеть (последняя модель в семинаре), оцените качество полученных эмбедингов в задании 1 и 2 (4 набора эмбедингов), также проверьте 1 любую из предобученных моделей с rus-vectores (но только не tayga_upos_skipgram_300_2_2019).\n",
        "Какая модель показывает наилучший результат?\n",
        "\n",
        "Убедитесь, что для каждой модели вы корректно воспроизводите пайплайн предобработки (в 1 задании у вас лемматизация, не забудьте ее применить к датасету для классификации; у выбранной предобученной модели может быть своя специфичная предобработка - ее нужно воспроизвести)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed908832",
      "metadata": {
        "id": "ed908832"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60c18c5a",
      "metadata": {
        "id": "60c18c5a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
