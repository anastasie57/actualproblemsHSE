{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1dba7c0d",
      "metadata": {
        "id": "1dba7c0d"
      },
      "source": [
        "# Домашнее задание № 10. Машинный перевод"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yj7aripVIsbG",
      "metadata": {
        "id": "Yj7aripVIsbG"
      },
      "source": [
        "## Задание 1 (6 баллов + 2 доп балла).\n",
        "Нужно обучить трансформер на этом же или на другом корпусе (можно взять другую языковую пару с того же сайте) и оценивать его на всей тестовой выборке (а не на 10 примерах как сделал я). Используйте метрику BLEU. Найдите лучшие (как минимум 5) переводы согласно этой метрике и проверьте действительно ли они хорошие. Если все переводы нулевые, то пообучайте модель подольше.\n",
        "\n",
        "Чтобы получить 2 доп балла вам нужно будет придумать как оптимизировать функцию translate. Сейчас она работает только с одним текстом - это не эффективно. Можно генерировать переводы сразу для нескольких текстов (батча). Главная сложность с таким подходом состоит в том, что генерируемые тексты будут заканчиваться в разное время и нужно сделать столько итераций, сколько нужно для завершения всех текстов (т.е. условие на то, что последний токен не равен [END] в текущем коде не сработает).\n",
        "ВАЖНО - недостаточно просто изменить входной аргумент с text на texts и добавить еще один цикл по texts! Сама модель должна вызываться на нескольких текстах!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "05d202c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05d202c4",
        "outputId": "b02613f7-e804-4112-bbe7-329a62742d9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.0.1\n",
            "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m784.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.0.0 (from torch==2.0.1)\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.43.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.27.9)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1)\n",
            "  Downloading lit-18.1.2.tar.gz (161 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.0/161.0 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "Building wheels for collected packages: lit\n",
            "  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-18.1.2-py3-none-any.whl size=96368 sha256=9c07dcfe71acb2f439bd4cbba525c2d3b0b9308f5d75fee1a4f26fc8d0aa5b06\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/4d/9c/3e28d23c2c6fc6a9bd89c91a7b7ff775fc71a41ac9a52563e9\n",
            "Successfully built lit\n",
            "Installing collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.2.0\n",
            "    Uninstalling triton-2.2.0:\n",
            "      Successfully uninstalled triton-2.2.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.1+cu121\n",
            "    Uninstalling torch-2.2.1+cu121:\n",
            "      Successfully uninstalled torch-2.2.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.0.1 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-18.1.2 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 triton-2.0.0\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting keras\n",
            "  Downloading keras-3.1.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\n",
            "Collecting namex (from keras)\n",
            "  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\n",
            "Collecting optree (from keras)\n",
            "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.10.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Installing collected packages: namex, optree, keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-3.1.1 namex-0.0.7 optree-0.11.0\n"
          ]
        }
      ],
      "source": [
        "# В колабе установите другую версию торча, чтобы семинарская тетрадка работала\n",
        "!pip install torch==2.0.1\n",
        "!pip install keras -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "CVpm_5MtTHiV",
      "metadata": {
        "id": "CVpm_5MtTHiV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19d4b003-0bda-466f-d0d7-82b32e3a9fbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.1.1\n",
            "2.0.1+cu117\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
        "\n",
        "import torch\n",
        "import keras\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers import normalizers\n",
        "from tokenizers.normalizers import Lowercase\n",
        "from tokenizers.trainers import WordPieceTrainer\n",
        "from tokenizers import decoders\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "print(keras.__version__)\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "duZ0j02oTv3m",
      "metadata": {
        "id": "duZ0j02oTv3m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f26be47-ccf0-43d9-e62c-24a28144defb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-06 19:58:36--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 121340806 (116M)\n",
            "Saving to: ‘opus.en-ru-train.ru’\n",
            "\n",
            "opus.en-ru-train.ru 100%[===================>] 115.72M  20.4MB/s    in 6.4s    \n",
            "\n",
            "2024-04-06 19:58:43 (18.0 MB/s) - ‘opus.en-ru-train.ru’ saved [121340806/121340806]\n",
            "\n",
            "--2024-04-06 19:58:43--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 67760131 (65M)\n",
            "Saving to: ‘opus.en-ru-train.en’\n",
            "\n",
            "opus.en-ru-train.en 100%[===================>]  64.62M  18.0MB/s    in 4.0s    \n",
            "\n",
            "2024-04-06 19:58:47 (16.0 MB/s) - ‘opus.en-ru-train.en’ saved [67760131/67760131]\n",
            "\n",
            "--2024-04-06 19:58:47--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.ru\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 305669 (299K)\n",
            "Saving to: ‘opus.en-ru-test.ru’\n",
            "\n",
            "opus.en-ru-test.ru  100%[===================>] 298.50K   542KB/s    in 0.6s    \n",
            "\n",
            "2024-04-06 19:58:49 (542 KB/s) - ‘opus.en-ru-test.ru’ saved [305669/305669]\n",
            "\n",
            "--2024-04-06 19:58:49--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.en\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 173307 (169K)\n",
            "Saving to: ‘opus.en-ru-test.en’\n",
            "\n",
            "opus.en-ru-test.en  100%[===================>] 169.25K   407KB/s    in 0.4s    \n",
            "\n",
            "2024-04-06 19:58:50 (407 KB/s) - ‘opus.en-ru-test.en’ saved [173307/173307]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.ru\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "75PRepBWUKXm",
      "metadata": {
        "id": "75PRepBWUKXm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1796c01b-cfa2-4539-8122-51a1006f2c75"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('so what are you thinking?', 'ну и что ты думаешь?')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "text = open('/content/opus.en-ru-train.ru').read().replace('\\xa0', ' ')\n",
        "f = open('/content/opus.en-ru-train.ru', 'w')\n",
        "f.write(text)\n",
        "f.close()\n",
        "\n",
        "en_sents = open('/content/opus.en-ru-train.en').read().lower().splitlines()\n",
        "ru_sents = open('/content/opus.en-ru-train.ru').read().lower().splitlines()\n",
        "en_sents[-1], ru_sents[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fTC5goRHWt8f",
      "metadata": {
        "id": "fTC5goRHWt8f"
      },
      "outputs": [],
      "source": [
        "tokenizer_en = Tokenizer(WordPiece(), )\n",
        "tokenizer_en.normalizer = normalizers.Sequence([Lowercase()])\n",
        "tokenizer_en.pre_tokenizer = Whitespace()\n",
        "\n",
        "trainer_en = WordPieceTrainer(\n",
        "          vocab_size=30000, special_tokens=[\"[UNK]\", \"[PAD]\"])\n",
        "tokenizer_en.train(files=[\"opus.en-ru-train.en\"], trainer=trainer_en )\n",
        "\n",
        "tokenizer_ru = Tokenizer(WordPiece(), )\n",
        "tokenizer_ru.normalizer = normalizers.Sequence([Lowercase()])\n",
        "tokenizer_ru.pre_tokenizer = Whitespace()\n",
        "\n",
        "trainer_ru = WordPieceTrainer(\n",
        "          vocab_size=30000, special_tokens=[\"[UNK]\", \"[PAD]\", \"[START]\", \"[END]\", ])\n",
        "tokenizer_ru.train(files=[\"opus.en-ru-train.ru\"], trainer=trainer_ru )\n",
        "\n",
        "tokenizer_en.decoder = decoders.WordPiece()\n",
        "tokenizer_ru.decoder = decoders.WordPiece()\n",
        "\n",
        "# раскоментируйте эту ячейку при обучении токенизатора\n",
        "# а потом снова закоментируйте чтобы при перезапуске не перезаписать токенизаторы\n",
        "tokenizer_en.save('tokenizer_en')\n",
        "tokenizer_ru.save('tokenizer_ru')\n",
        "\n",
        "tokenizer_en = Tokenizer.from_file(\"tokenizer_en\")\n",
        "tokenizer_ru = Tokenizer.from_file(\"tokenizer_ru\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "aG8eaMJZXFoz",
      "metadata": {
        "id": "aG8eaMJZXFoz"
      },
      "outputs": [],
      "source": [
        "def encode(text, tokenizer, target=False):\n",
        "    if target:\n",
        "        return [tokenizer.token_to_id('[START]')] + tokenizer.encode(text).ids + \\\n",
        "                [tokenizer.token_to_id('[END]')]\n",
        "    else:\n",
        "        return tokenizer.encode(text).ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "79cWd3jXXWCY",
      "metadata": {
        "id": "79cWd3jXXWCY"
      },
      "outputs": [],
      "source": [
        "X_en = [encode(t, tokenizer_en) for t in en_sents]\n",
        "X_ru = [encode(t, tokenizer_ru, True) for t in ru_sents]\n",
        "\n",
        "max_len_en = np.max([len(x) for x in X_en])\n",
        "max_len_ru = np.max([len(x) for x in X_ru])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "FumWvBIaXZBs",
      "metadata": {
        "id": "FumWvBIaXZBs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88125c49-4d06-4a96-cf07-da2d7230b674"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17863, 19389)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "max_len_en, max_len_ru"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "X13JyWc8Y-Pi",
      "metadata": {
        "id": "X13JyWc8Y-Pi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a5b13c1-d8ff-417b-ecda-86c34ba53596"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Исходный язык\n",
            "50 % текстов <=  10.0\n",
            "75 % текстов <=  18.0\n",
            "90 % текстов <=  33.0\n",
            "95 % текстов <=  45.0\n",
            "99 % текстов <=  75.0\n"
          ]
        }
      ],
      "source": [
        "print('Исходный язык')\n",
        "print('50 % текстов <= ', np.percentile([len(x) for x in X_en], 50))\n",
        "print('75 % текстов <= ', np.percentile([len(x) for x in X_en], 75))\n",
        "print('90 % текстов <= ', np.percentile([len(x) for x in X_en], 90))\n",
        "print('95 % текстов <= ', np.percentile([len(x) for x in X_en], 95))\n",
        "print('99 % текстов <= ', np.percentile([len(x) for x in X_en], 99))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "j2hSt-yFZBe9",
      "metadata": {
        "id": "j2hSt-yFZBe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bd9413d-6070-4ce8-8c37-395bdf7d92fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Целевой язык\n",
            "50 % текстов <=  11.0\n",
            "75 % текстов <=  20.0\n",
            "90 % текстов <=  36.0\n",
            "95 % текстов <=  48.0\n",
            "99 % текстов <=  79.0\n"
          ]
        }
      ],
      "source": [
        "print('Целевой язык')\n",
        "print('50 % текстов <= ', np.percentile([len(x) for x in X_ru], 50))\n",
        "print('75 % текстов <= ', np.percentile([len(x) for x in X_ru], 75))\n",
        "print('90 % текстов <= ', np.percentile([len(x) for x in X_ru], 90))\n",
        "print('95 % текстов <= ', np.percentile([len(x) for x in X_ru], 95))\n",
        "print('99 % текстов <= ', np.percentile([len(x) for x in X_ru], 99))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "-4NWKGN7ZFOr",
      "metadata": {
        "id": "-4NWKGN7ZFOr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d267a6b1-8a15-4116-bb66-6c2a4702bf6d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "max_len_en, max_len_ru = 45, 48\n",
        "\n",
        "PAD_IDX = tokenizer_ru.token_to_id('[PAD]')\n",
        "PAD_IDX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "iGczKpOTZK0c",
      "metadata": {
        "id": "iGczKpOTZK0c"
      },
      "outputs": [],
      "source": [
        "X_en = keras.preprocessing.sequence.pad_sequences(\n",
        "              X_en, maxlen=max_len_en, padding='post', value=PAD_IDX)\n",
        "\n",
        "X_ru_out = keras.preprocessing.sequence.pad_sequences(\n",
        "              [x[1:] for x in X_ru], maxlen=max_len_ru-1, padding='post',\n",
        "              value=PAD_IDX)\n",
        "\n",
        "X_ru_dec = keras.preprocessing.sequence.pad_sequences(\n",
        "              [x[:-1] for x in X_ru], maxlen=max_len_ru-1,\n",
        "              padding='post', value=PAD_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "EJ20-UDxZR7q",
      "metadata": {
        "id": "EJ20-UDxZR7q"
      },
      "outputs": [],
      "source": [
        "(X_en_train, X_en_valid,\n",
        "X_ru_dec_train, X_ru_dec_valid,\n",
        "X_ru_out_train, X_ru_out_valid) = train_test_split(X_en,\n",
        "                                                  X_ru_dec,\n",
        "                                                  X_ru_out,\n",
        "                                                  test_size=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "r-j2Pd2WZXCY",
      "metadata": {
        "id": "r-j2Pd2WZXCY"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "    # Считаем скалярное произведение между запросом (query) и ключом (key), транспонируя ключ\n",
        "    matmul_qk = keras.ops.matmul(query, keras.ops.transpose(key, axes=[0, 1, 3, 2]))\n",
        "\n",
        "    # Получаем глубину (размерность) ключа и преобразуем ее во float\n",
        "    depth = keras.ops.cast(key.shape[-1], torch.float32)\n",
        "\n",
        "    # Делим результат скалярного произведения на квадратный корень из глубины\n",
        "    # Это делается для уменьшения влияния больших значений и стабилизации градиентов во время обучения\n",
        "    logits = matmul_qk / keras.ops.sqrt(depth)\n",
        "\n",
        "    # Если есть маска, применяем ее к логитам, чтобы обнулить нежелательные значения\n",
        "    if mask is not None:\n",
        "        logits += (mask * -1e9)\n",
        "\n",
        "    # Применяем функцию softmax для получения весов внимания\n",
        "    attention_weights = keras.ops.softmax(logits, axis=-1)\n",
        "\n",
        "    # Умножаем веса внимания на значения (value) для получения итогового результата\n",
        "    output = keras.ops.matmul(attention_weights, value)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadAttention(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "        super(MultiHeadAttention, self).__init__(name=name)\n",
        "        self.num_heads = num_heads  # количество голов для внимания\n",
        "        self.d_model = d_model  # размерность вектора модели\n",
        "\n",
        "        # Убеждаемся, что размерность модели делится нацело на количество голов\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads  # размерность каждой головы\n",
        "\n",
        "        # Создаем полносвязные слои для запроса, ключа и значения\n",
        "        self.query_dense = keras.layers.Dense(units=d_model)\n",
        "        self.key_dense = keras.layers.Dense(units=d_model)\n",
        "        self.value_dense = keras.layers.Dense(units=d_model)\n",
        "\n",
        "        # Создаем последний полносвязный слой\n",
        "        self.dense = keras.layers.Dense(units=d_model)\n",
        "\n",
        "    def split_heads(self, inputs, batch_size):\n",
        "        # Разделяем входные данные на головы\n",
        "        inputs = keras.ops.reshape(\n",
        "            inputs, newshape=(batch_size, -1, self.num_heads, self.depth))\n",
        "        return keras.ops.transpose(inputs, axes=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
        "            'value'], inputs.get('mask', None)\n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        # Пропускаем запрос, ключ и значение через соответствующие полносвязные слои\n",
        "        query = self.query_dense(query)\n",
        "        key = self.key_dense(key)\n",
        "        value = self.value_dense(value)\n",
        "\n",
        "        # Разделяем запрос, ключ и значение на головы\n",
        "        # то есть просто разрезаем вектора на num_heads частей\n",
        "        # и сравниваем все части между собой\n",
        "        query = self.split_heads(query, batch_size)\n",
        "        key = self.split_heads(key, batch_size)\n",
        "        value = self.split_heads(value, batch_size)\n",
        "\n",
        "        # Выполняем механизм внимания с масштабированным скалярным произведением\n",
        "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
        "\n",
        "        scaled_attention = keras.ops.transpose(scaled_attention, axes=[0, 2, 1, 3])\n",
        "\n",
        "        # Объединяем головы вместе (склеиваем векторы в один)\n",
        "        concat_attention = keras.ops.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))\n",
        "\n",
        "        # Пропускаем объединенное внимание через дополнительный полносвязный слой\n",
        "        # Он просто добавляет сложности нашей модели\n",
        "        outputs = self.dense(concat_attention)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "\n",
        "def create_padding_mask(x):\n",
        "    mask = keras.ops.cast(keras.ops.equal(x, PAD_IDX), torch.float32)\n",
        "    # (batch_size, 1, 1, sequence length)\n",
        "    return mask[:, None, None, :]\n",
        "\n",
        "\n",
        "def create_look_ahead_mask(x):\n",
        "    # эта функция немножко сложная, но суть у нее достаточно простая\n",
        "    # нужно создать треугольную маску, с помощью которой мы закроем\n",
        "    # для каждого токена все последующие токены\n",
        "    seq_len = x.shape[1]\n",
        "    ones_mask = keras.ops.ones((1, seq_len, seq_len), dtype=\"int32\")\n",
        "    row_index = keras.ops.cumsum(ones_mask, axis=-2)\n",
        "    col_index = keras.ops.cumsum(ones_mask, axis=-1)\n",
        "    look_ahead_mask = ~ keras.ops.greater_equal(row_index, col_index)\n",
        "    padding_mask = create_padding_mask(x)\n",
        "    return keras.ops.maximum(look_ahead_mask, padding_mask)\n",
        "\n",
        "\n",
        "class PositionalEncoding(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, position, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "    def get_angles(self, position, i, d_model):\n",
        "        angles = 1 / keras.ops.power(10000, (2 * (i // 2)) / d_model)\n",
        "        return keras.ops.multiply(position, angles)\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(\n",
        "            position=torch.arange(0, position, dtype=torch.float32)[:, None],\n",
        "            i=torch.arange(0, d_model, dtype=torch.float32)[None, :],\n",
        "            d_model=d_model)\n",
        "        sines = keras.ops.sin(angle_rads[:, 0::2])\n",
        "        cosines = keras.ops.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = keras.ops.concatenate([sines, cosines], axis=-1)\n",
        "        pos_encoding = pos_encoding[None, ...]\n",
        "        return keras.ops.cast(pos_encoding, torch.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs + self.pos_encoding[:, :inputs.shape[1], :]\n",
        "\n",
        "\n",
        "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
        "    inputs = keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "    padding_mask = keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "    #call_mha\n",
        "    attention = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention\")({\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "    attention = keras.layers.Dropout(rate=dropout)(attention)\n",
        "    attention = keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "    outputs = keras.layers.Dense(units=units, activation='relu')(attention)\n",
        "    outputs = keras.layers.Dense(units=d_model)(outputs)\n",
        "    outputs = keras.layers.Dropout(rate=dropout)(outputs)\n",
        "    outputs = keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "    return keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
        "\n",
        "\n",
        "def encoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            max_len,\n",
        "            name=\"encoder\"):\n",
        "    inputs = keras.Input(shape=(max_len,), name=\"inputs\")\n",
        "    padding_mask = keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "    embeddings = keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "    embeddings *= keras.ops.sqrt(keras.ops.cast(d_model, torch.float32))\n",
        "    embeddings = PositionalEncoding(max_len, d_model)(embeddings)\n",
        "\n",
        "    #inputs (они тут называются outputs но это просто такой нейминг,\n",
        "    # этот параметр передается в encoder_layer первым и encoder_layer будет считать его inputs)\n",
        "    # outputs он тут называется для удобства, так как он будет перезаписываться\n",
        "    # чтобы на вход следующему блоку подавать уже не эмбединги,\n",
        "    # а то что получится как результат предыдущего блока\n",
        "    outputs = keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        outputs = encoder_layer(\n",
        "            units=units,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            name=\"encoder_layer_{}\".format(i),\n",
        "        )([outputs, padding_mask])\n",
        "\n",
        "    return keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
        "\n",
        "\n",
        "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
        "    inputs = keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "    enc_outputs = keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
        "    look_ahead_mask = keras.Input(\n",
        "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
        "    padding_mask = keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "    attention1 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': look_ahead_mask\n",
        "      })\n",
        "    attention1 = keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention1 + inputs)\n",
        "\n",
        "    attention2 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
        "          'query': attention1,\n",
        "          'key': enc_outputs,\n",
        "          'value': enc_outputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "    attention2 = keras.layers.Dropout(rate=dropout)(attention2)\n",
        "    attention2 = keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention2 + attention1)\n",
        "\n",
        "    outputs = keras.layers.Dense(units=units, activation='relu')(attention2)\n",
        "    outputs = keras.layers.Dense(units=d_model)(outputs)\n",
        "    outputs = keras.layers.Dropout(rate=dropout)(outputs)\n",
        "    outputs = keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(outputs + attention2)\n",
        "\n",
        "    return keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6jO1ZczuaS2h",
      "metadata": {
        "id": "6jO1ZczuaS2h"
      },
      "outputs": [],
      "source": [
        "def decoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            max_len,\n",
        "            name='decoder'):\n",
        "    inputs = keras.Input(shape=(max_len,), name='inputs')\n",
        "    enc_outputs = keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
        "    look_ahead_mask = keras.Input(\n",
        "      shape=(1, None, None), name='look_ahead_mask')\n",
        "    padding_mask = keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "    embeddings = keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "    embeddings *= keras.ops.sqrt(keras.ops.cast(d_model, torch.float32))\n",
        "    embeddings = PositionalEncoding(max_len, d_model)(embeddings)\n",
        "\n",
        "    outputs = keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        outputs = decoder_layer(\n",
        "            units=units,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            name='decoder_layer_{}'.format(i),\n",
        "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
        "\n",
        "    return keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "CIn4mFJbah9a",
      "metadata": {
        "id": "CIn4mFJbah9a"
      },
      "outputs": [],
      "source": [
        "def transformer(vocab_size,\n",
        "                num_layers,\n",
        "                units,\n",
        "                d_model,\n",
        "                num_heads,\n",
        "                dropout,\n",
        "                max_len,\n",
        "                name=\"transformer\"):\n",
        "    inputs = keras.Input(shape=(max_len[0],), name=\"inputs\")\n",
        "    dec_inputs = keras.Input(shape=(max_len[1]-1,), name=\"dec_inputs\")\n",
        "\n",
        "    enc_padding_mask = keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='enc_padding_mask')(inputs)\n",
        "\n",
        "    look_ahead_mask = keras.layers.Lambda(\n",
        "      create_look_ahead_mask,\n",
        "      output_shape=(1, None, None),\n",
        "      name='look_ahead_mask')(dec_inputs)\n",
        "\n",
        "    dec_padding_mask = keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='dec_padding_mask')(inputs)\n",
        "\n",
        "    enc_outputs = encoder(\n",
        "                          vocab_size=vocab_size[0],\n",
        "                          num_layers=num_layers,\n",
        "                          units=units,\n",
        "                          d_model=d_model,\n",
        "                          num_heads=num_heads,\n",
        "                          dropout=dropout,\n",
        "                          max_len=max_len[0],\n",
        "                        )(inputs=[inputs, enc_padding_mask])\n",
        "\n",
        "    dec_outputs = decoder(\n",
        "                          vocab_size=vocab_size[1],\n",
        "                          num_layers=num_layers,\n",
        "                          units=units,\n",
        "                          d_model=d_model,\n",
        "                          num_heads=num_heads,\n",
        "                          dropout=dropout,\n",
        "                          max_len=max_len[1]-1,\n",
        "                        )(inputs=[dec_inputs, enc_outputs,\n",
        "                                  look_ahead_mask, dec_padding_mask])\n",
        "\n",
        "    outputs = keras.layers.Dense(units=vocab_size[1], name=\"outputs\")(dec_outputs)\n",
        "\n",
        "    return keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "8P91cgcja0LT",
      "metadata": {
        "id": "8P91cgcja0LT"
      },
      "outputs": [],
      "source": [
        "L  = keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True, reduction='none',)\n",
        "\n",
        "def loss_function(y_true, y_pred):\n",
        "    loss = L(y_true, y_pred)\n",
        "\n",
        "    mask = keras.ops.cast(keras.ops.not_equal(y_true, PAD_IDX), torch.float32)\n",
        "    loss = keras.ops.multiply(loss, mask)\n",
        "\n",
        "    return keras.ops.mean(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "C4roupG-bAe6",
      "metadata": {
        "id": "C4roupG-bAe6"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "# small model\n",
        "NUM_LAYERS = 2\n",
        "D_MODEL = 256\n",
        "NUM_HEADS = 8\n",
        "\n",
        "UNITS = 512\n",
        "DROPOUT = 0.1\n",
        "\n",
        "\n",
        "# average model\n",
        "# NUM_LAYERS = 6\n",
        "# D_MODEL = 512\n",
        "# NUM_HEADS = 8\n",
        "# UNITS = 2048\n",
        "# DROPOUT = 0.1\n",
        "\n",
        "\n",
        "model = transformer(\n",
        "    vocab_size=(tokenizer_en.get_vocab_size(),tokenizer_ru.get_vocab_size()),\n",
        "    num_layers=NUM_LAYERS,\n",
        "    units=UNITS,\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    dropout=DROPOUT,\n",
        "    max_len=[max_len_en, max_len_ru])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(\n",
        "    0.001, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    return keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
        "checkpoint = keras.callbacks.ModelCheckpoint('model_ruen.weights.h5',\n",
        "                                            monitor='val_loss',\n",
        "                                            verbose=1,\n",
        "                                            save_weights_only=True,\n",
        "                                            save_best_only=True,\n",
        "                                            mode='min',\n",
        "                                            save_freq='epoch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "nBJGyj1FbI6K",
      "metadata": {
        "id": "nBJGyj1FbI6K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8dfae5d-aace-4800-c0df-e56cabbce53b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "Rlsofmo-byOZ",
      "metadata": {
        "id": "Rlsofmo-byOZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6108f50-ffa8-40bf-e044-3ca34843016e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m4750/4750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346ms/step - accuracy: 0.0892 - loss: 1.6180\n",
            "Epoch 1: val_loss improved from inf to 0.98561, saving model to model_ruen.weights.h5\n",
            "\u001b[1m4750/4750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1692s\u001b[0m 356ms/step - accuracy: 0.0892 - loss: 1.6179 - val_accuracy: 0.1465 - val_loss: 0.9856\n",
            "Epoch 2/2\n",
            "\u001b[1m4750/4750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347ms/step - accuracy: 0.1478 - loss: 0.9666\n",
            "Epoch 2: val_loss improved from 0.98561 to 0.88292, saving model to model_ruen.weights.h5\n",
            "\u001b[1m4750/4750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1694s\u001b[0m 357ms/step - accuracy: 0.1478 - loss: 0.9666 - val_accuracy: 0.1589 - val_loss: 0.8829\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a2c58f133a0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "model.fit((X_en_train, X_ru_dec_train), X_ru_out_train,\n",
        "             validation_data=((X_en_valid, X_ru_dec_valid), X_ru_out_valid),\n",
        "             batch_size=200,\n",
        "             epochs=2,\n",
        "             callbacks=[checkpoint]\n",
        "             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "WSuhUa5ccHHz",
      "metadata": {
        "id": "WSuhUa5ccHHz"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def translate(text):\n",
        "    input_ids = encode(text, tokenizer_en, target=False)\n",
        "\n",
        "    input_ids = keras.ops.cast(keras.preprocessing.sequence.pad_sequences(\n",
        "                                      [input_ids], maxlen=max_len_en, padding='post',\n",
        "                                       # важно не забыть паддинг с нужным id\n",
        "                                       value=PAD_IDX), torch.int32)\n",
        "\n",
        "\n",
        "\n",
        "    output_ids = [tokenizer_ru.token_to_id('[START]') ]\n",
        "\n",
        "    pred = model((input_ids, keras.ops.cast(keras.preprocessing.sequence.pad_sequences(\n",
        "                                      [output_ids], maxlen=max_len_ru-1, padding='post',\n",
        "                                       # важно не забыть паддинг с нужным id\n",
        "                                       value=PAD_IDX), torch.int32))).cpu().numpy()\n",
        "\n",
        "\n",
        "    while pred.argmax(2)[0][-1] not in [tokenizer_ru.token_to_id('[END]')]:\n",
        "        if len(output_ids) >= max_len_ru:\n",
        "            break\n",
        "        # можно занизить скор тэга UNK чтобы он никогда не генерировался\n",
        "        pred[:, :, tokenizer_ru.token_to_id('[UNK]')] = -100\n",
        "\n",
        "        output_ids.append(pred.argmax(2)[0][-1])\n",
        "        pred = model((input_ids, keras.ops.cast(keras.preprocessing.sequence.pad_sequences(\n",
        "                                      [output_ids], maxlen=max_len_ru-1, padding='post',\n",
        "                                       # важно не забыть паддинг с нужным id\n",
        "                                       value=PAD_IDX), torch.int32))).cpu().numpy()\n",
        "\n",
        "    return tokenizer_ru.decode(output_ids[1:], )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "M6Zhx1uGcLBh",
      "metadata": {
        "id": "M6Zhx1uGcLBh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "10433796-f625-4342-917e-020e2dc13bc8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'вы можете перевести это?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "translate(\"can you translate this sentence?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "lRqjzAhBrYz2",
      "metadata": {
        "id": "lRqjzAhBrYz2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d42f9f08-8867-4019-8ff3-9c25b1c72137"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.3601999999999999"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "text = open('opus.en-ru-test.ru').read().replace('\\xa0', ' ')\n",
        "f = open('opus.en-ru-test.ru', 'w')\n",
        "f.write(text)\n",
        "f.close()\n",
        "\n",
        "en_sents_test = open('opus.en-ru-test.en').read().lower().splitlines()\n",
        "ru_sents_test = open('opus.en-ru-test.ru').read().lower().splitlines()\n",
        "\n",
        "translations = []\n",
        "\n",
        "for i in en_sents_test:\n",
        "    translations.append(translate(i))\n",
        "\n",
        "bleus = []\n",
        "\n",
        "for i, t in enumerate(translations):\n",
        "    reference = tokenizer_ru.encode(t).tokens\n",
        "    hypothesis = tokenizer_ru.encode(ru_sents_test[i]).tokens\n",
        "\n",
        "    bleus.append(round(nltk.translate.bleu_score.sentence_bleu([reference], hypothesis,  auto_reweigh=True), 3))\n",
        "\n",
        "(sum(bleus)/len(bleus))*100"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5aa93d6",
      "metadata": {
        "id": "b5aa93d6"
      },
      "source": [
        "\n",
        "## Задание 2 (2 балла).\n",
        "Прочитайте главу про машинный перевод у Журафски и Маннига - https://web.stanford.edu/~jurafsky/slp3/13.pdf\n",
        "Ответьте своими словами в чем заключается техника back translation? Для чего она применяется и что позволяет получить? Опишите по шагам как ее применить к паре en->ru на данных из семинара. Сколько моделей понадобится? Сколько запусков обучения нужно будет сделать?\n",
        "\n",
        "Ответ должен содержать как минимум 10 предложений."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8iVWiHwAHDZp",
      "metadata": {
        "id": "8iVWiHwAHDZp"
      },
      "source": [
        "Подход backtranslation можно использовать, когда у нас есть только небольшой объём параллельного двуязычного корпуса. Мы можем синтетически увеличить его объём, если у нас есть большой корпус на целевом языке. Для этого мы сначала обучаем модель перевода с целевого языка на исходный язык, переводим предложения из моноязычного корпуса на целевом языке на тот язык, который был исходным. Таким образом мы получаем увеличенный двуязычный корпус, на котором можем дальше обучить переводчик с исходного языка на целевой.\n",
        "\n",
        "Таким образом, для реализации перевода с английского на русский мы сначала должны обучить модель для перевода с русского на английский, перевести дополнительный набор предложений с помощью этой модели, а затем, используя исходный корпус и свежепереведённые предложения, обучить вторую модель уже для перевода на русский."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
